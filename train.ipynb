{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the training, test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datasets\n",
    "from collections import Counter\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['comment_id', 'annotator_id', 'platform', 'sentiment', 'respect', 'insult', 'humiliate', 'status', 'dehumanize', 'violence', 'genocide', 'attack_defend', 'hatespeech', 'hate_speech_score', 'text', 'infitms', 'outfitms', 'annotator_severity', 'std_err', 'annotator_infitms', 'annotator_outfitms', 'hypothesis', 'target_race_asian', 'target_race_black', 'target_race_latinx', 'target_race_middle_eastern', 'target_race_native_american', 'target_race_pacific_islander', 'target_race_white', 'target_race_other', 'target_race', 'target_religion_atheist', 'target_religion_buddhist', 'target_religion_christian', 'target_religion_hindu', 'target_religion_jewish', 'target_religion_mormon', 'target_religion_muslim', 'target_religion_other', 'target_religion', 'target_origin_immigrant', 'target_origin_migrant_worker', 'target_origin_specific_country', 'target_origin_undocumented', 'target_origin_other', 'target_origin', 'target_gender_men', 'target_gender_non_binary', 'target_gender_transgender_men', 'target_gender_transgender_unspecified', 'target_gender_transgender_women', 'target_gender_women', 'target_gender_other', 'target_gender', 'target_sexuality_bisexual', 'target_sexuality_gay', 'target_sexuality_lesbian', 'target_sexuality_straight', 'target_sexuality_other', 'target_sexuality', 'target_age_children', 'target_age_teenagers', 'target_age_young_adults', 'target_age_middle_aged', 'target_age_seniors', 'target_age_other', 'target_age', 'target_disability_physical', 'target_disability_cognitive', 'target_disability_neurological', 'target_disability_visually_impaired', 'target_disability_hearing_impaired', 'target_disability_unspecific', 'target_disability_other', 'target_disability', 'annotator_gender', 'annotator_trans', 'annotator_educ', 'annotator_income', 'annotator_ideology', 'annotator_gender_men', 'annotator_gender_women', 'annotator_gender_non_binary', 'annotator_gender_prefer_not_to_say', 'annotator_gender_self_describe', 'annotator_transgender', 'annotator_cisgender', 'annotator_transgender_prefer_not_to_say', 'annotator_education_some_high_school', 'annotator_education_high_school_grad', 'annotator_education_some_college', 'annotator_education_college_grad_aa', 'annotator_education_college_grad_ba', 'annotator_education_professional_degree', 'annotator_education_masters', 'annotator_education_phd', 'annotator_income_<10k', 'annotator_income_10k-50k', 'annotator_income_50k-100k', 'annotator_income_100k-200k', 'annotator_income_>200k', 'annotator_ideology_extremeley_conservative', 'annotator_ideology_conservative', 'annotator_ideology_slightly_conservative', 'annotator_ideology_neutral', 'annotator_ideology_slightly_liberal', 'annotator_ideology_liberal', 'annotator_ideology_extremeley_liberal', 'annotator_ideology_no_opinion', 'annotator_race_asian', 'annotator_race_black', 'annotator_race_latinx', 'annotator_race_middle_eastern', 'annotator_race_native_american', 'annotator_race_pacific_islander', 'annotator_race_white', 'annotator_race_other', 'annotator_age', 'annotator_religion_atheist', 'annotator_religion_buddhist', 'annotator_religion_christian', 'annotator_religion_hindu', 'annotator_religion_jewish', 'annotator_religion_mormon', 'annotator_religion_muslim', 'annotator_religion_nothing', 'annotator_religion_other', 'annotator_sexuality_bisexual', 'annotator_sexuality_gay', 'annotator_sexuality_straight', 'annotator_sexuality_other'],\n",
      "        num_rows: 135556\n",
      "    })\n",
      "})\n",
      "{'train': (135556, 131)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'comment_id': 47777,\n",
       " 'annotator_id': 10873,\n",
       " 'platform': 3,\n",
       " 'sentiment': 0.0,\n",
       " 'respect': 0.0,\n",
       " 'insult': 0.0,\n",
       " 'humiliate': 0.0,\n",
       " 'status': 2.0,\n",
       " 'dehumanize': 0.0,\n",
       " 'violence': 0.0,\n",
       " 'genocide': 0.0,\n",
       " 'attack_defend': 0.0,\n",
       " 'hatespeech': 0.0,\n",
       " 'hate_speech_score': -3.9,\n",
       " 'text': 'Yes indeed. She sort of reminds me of the elder lady that played the part in the movie \"Titanic\" who was telling her story!!! And I wouldn\\'t have wanted to cover who I really am!! I would be proud!!!! WE should be proud of our race no matter what it is!!',\n",
       " 'infitms': 0.81,\n",
       " 'outfitms': 1.88,\n",
       " 'annotator_severity': 0.36,\n",
       " 'std_err': 0.34,\n",
       " 'annotator_infitms': 1.35,\n",
       " 'annotator_outfitms': 1.23,\n",
       " 'hypothesis': -1.1301777576839678,\n",
       " 'target_race_asian': True,\n",
       " 'target_race_black': True,\n",
       " 'target_race_latinx': True,\n",
       " 'target_race_middle_eastern': True,\n",
       " 'target_race_native_american': True,\n",
       " 'target_race_pacific_islander': True,\n",
       " 'target_race_white': True,\n",
       " 'target_race_other': False,\n",
       " 'target_race': True,\n",
       " 'target_religion_atheist': False,\n",
       " 'target_religion_buddhist': False,\n",
       " 'target_religion_christian': False,\n",
       " 'target_religion_hindu': False,\n",
       " 'target_religion_jewish': False,\n",
       " 'target_religion_mormon': False,\n",
       " 'target_religion_muslim': False,\n",
       " 'target_religion_other': False,\n",
       " 'target_religion': False,\n",
       " 'target_origin_immigrant': False,\n",
       " 'target_origin_migrant_worker': False,\n",
       " 'target_origin_specific_country': False,\n",
       " 'target_origin_undocumented': False,\n",
       " 'target_origin_other': False,\n",
       " 'target_origin': False,\n",
       " 'target_gender_men': False,\n",
       " 'target_gender_non_binary': False,\n",
       " 'target_gender_transgender_men': False,\n",
       " 'target_gender_transgender_unspecified': False,\n",
       " 'target_gender_transgender_women': False,\n",
       " 'target_gender_women': False,\n",
       " 'target_gender_other': False,\n",
       " 'target_gender': False,\n",
       " 'target_sexuality_bisexual': False,\n",
       " 'target_sexuality_gay': False,\n",
       " 'target_sexuality_lesbian': False,\n",
       " 'target_sexuality_straight': False,\n",
       " 'target_sexuality_other': False,\n",
       " 'target_sexuality': False,\n",
       " 'target_age_children': False,\n",
       " 'target_age_teenagers': False,\n",
       " 'target_age_young_adults': False,\n",
       " 'target_age_middle_aged': False,\n",
       " 'target_age_seniors': False,\n",
       " 'target_age_other': False,\n",
       " 'target_age': False,\n",
       " 'target_disability_physical': False,\n",
       " 'target_disability_cognitive': False,\n",
       " 'target_disability_neurological': False,\n",
       " 'target_disability_visually_impaired': False,\n",
       " 'target_disability_hearing_impaired': False,\n",
       " 'target_disability_unspecific': False,\n",
       " 'target_disability_other': False,\n",
       " 'target_disability': False,\n",
       " 'annotator_gender': 'male',\n",
       " 'annotator_trans': 'no',\n",
       " 'annotator_educ': 'college_grad_ba',\n",
       " 'annotator_income': '<10k',\n",
       " 'annotator_ideology': 'neutral',\n",
       " 'annotator_gender_men': True,\n",
       " 'annotator_gender_women': False,\n",
       " 'annotator_gender_non_binary': False,\n",
       " 'annotator_gender_prefer_not_to_say': False,\n",
       " 'annotator_gender_self_describe': False,\n",
       " 'annotator_transgender': False,\n",
       " 'annotator_cisgender': True,\n",
       " 'annotator_transgender_prefer_not_to_say': False,\n",
       " 'annotator_education_some_high_school': False,\n",
       " 'annotator_education_high_school_grad': False,\n",
       " 'annotator_education_some_college': False,\n",
       " 'annotator_education_college_grad_aa': False,\n",
       " 'annotator_education_college_grad_ba': True,\n",
       " 'annotator_education_professional_degree': False,\n",
       " 'annotator_education_masters': False,\n",
       " 'annotator_education_phd': False,\n",
       " 'annotator_income_<10k': True,\n",
       " 'annotator_income_10k-50k': False,\n",
       " 'annotator_income_50k-100k': False,\n",
       " 'annotator_income_100k-200k': False,\n",
       " 'annotator_income_>200k': False,\n",
       " 'annotator_ideology_extremeley_conservative': False,\n",
       " 'annotator_ideology_conservative': False,\n",
       " 'annotator_ideology_slightly_conservative': False,\n",
       " 'annotator_ideology_neutral': True,\n",
       " 'annotator_ideology_slightly_liberal': False,\n",
       " 'annotator_ideology_liberal': False,\n",
       " 'annotator_ideology_extremeley_liberal': False,\n",
       " 'annotator_ideology_no_opinion': False,\n",
       " 'annotator_race_asian': False,\n",
       " 'annotator_race_black': False,\n",
       " 'annotator_race_latinx': False,\n",
       " 'annotator_race_middle_eastern': False,\n",
       " 'annotator_race_native_american': False,\n",
       " 'annotator_race_pacific_islander': False,\n",
       " 'annotator_race_white': True,\n",
       " 'annotator_race_other': False,\n",
       " 'annotator_age': 25.0,\n",
       " 'annotator_religion_atheist': False,\n",
       " 'annotator_religion_buddhist': False,\n",
       " 'annotator_religion_christian': True,\n",
       " 'annotator_religion_hindu': False,\n",
       " 'annotator_religion_jewish': False,\n",
       " 'annotator_religion_mormon': False,\n",
       " 'annotator_religion_muslim': False,\n",
       " 'annotator_religion_nothing': False,\n",
       " 'annotator_religion_other': False,\n",
       " 'annotator_sexuality_bisexual': False,\n",
       " 'annotator_sexuality_gay': False,\n",
       " 'annotator_sexuality_straight': True,\n",
       " 'annotator_sexuality_other': False}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load a dataset form the HuggingFace Hub\n",
    "hate = datasets.load_dataset('ucberkeley-dlab/measuring-hate-speech')\n",
    "\n",
    "# check the dataset structure\n",
    "print(hate)\n",
    "\n",
    "# check the size of the dataset\n",
    "print(hate.shape)\n",
    "\n",
    "hate[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['comment_id', 'annotator_id', 'platform', 'sentiment', 'respect', 'insult', 'humiliate', 'status', 'dehumanize', 'violence', 'genocide', 'attack_defend', 'hatespeech', 'hate_speech_score', 'text', 'infitms', 'outfitms', 'annotator_severity', 'std_err', 'annotator_infitms', 'annotator_outfitms', 'hypothesis', 'target_race_asian', 'target_race_black', 'target_race_latinx', 'target_race_middle_eastern', 'target_race_native_american', 'target_race_pacific_islander', 'target_race_white', 'target_race_other', 'target_race', 'target_religion_atheist', 'target_religion_buddhist', 'target_religion_christian', 'target_religion_hindu', 'target_religion_jewish', 'target_religion_mormon', 'target_religion_muslim', 'target_religion_other', 'target_religion', 'target_origin_immigrant', 'target_origin_migrant_worker', 'target_origin_specific_country', 'target_origin_undocumented', 'target_origin_other', 'target_origin', 'target_gender_men', 'target_gender_non_binary', 'target_gender_transgender_men', 'target_gender_transgender_unspecified', 'target_gender_transgender_women', 'target_gender_women', 'target_gender_other', 'target_gender', 'target_sexuality_bisexual', 'target_sexuality_gay', 'target_sexuality_lesbian', 'target_sexuality_straight', 'target_sexuality_other', 'target_sexuality', 'target_age_children', 'target_age_teenagers', 'target_age_young_adults', 'target_age_middle_aged', 'target_age_seniors', 'target_age_other', 'target_age', 'target_disability_physical', 'target_disability_cognitive', 'target_disability_neurological', 'target_disability_visually_impaired', 'target_disability_hearing_impaired', 'target_disability_unspecific', 'target_disability_other', 'target_disability', 'annotator_gender', 'annotator_trans', 'annotator_educ', 'annotator_income', 'annotator_ideology', 'annotator_gender_men', 'annotator_gender_women', 'annotator_gender_non_binary', 'annotator_gender_prefer_not_to_say', 'annotator_gender_self_describe', 'annotator_transgender', 'annotator_cisgender', 'annotator_transgender_prefer_not_to_say', 'annotator_education_some_high_school', 'annotator_education_high_school_grad', 'annotator_education_some_college', 'annotator_education_college_grad_aa', 'annotator_education_college_grad_ba', 'annotator_education_professional_degree', 'annotator_education_masters', 'annotator_education_phd', 'annotator_income_<10k', 'annotator_income_10k-50k', 'annotator_income_50k-100k', 'annotator_income_100k-200k', 'annotator_income_>200k', 'annotator_ideology_extremeley_conservative', 'annotator_ideology_conservative', 'annotator_ideology_slightly_conservative', 'annotator_ideology_neutral', 'annotator_ideology_slightly_liberal', 'annotator_ideology_liberal', 'annotator_ideology_extremeley_liberal', 'annotator_ideology_no_opinion', 'annotator_race_asian', 'annotator_race_black', 'annotator_race_latinx', 'annotator_race_middle_eastern', 'annotator_race_native_american', 'annotator_race_pacific_islander', 'annotator_race_white', 'annotator_race_other', 'annotator_age', 'annotator_religion_atheist', 'annotator_religion_buddhist', 'annotator_religion_christian', 'annotator_religion_hindu', 'annotator_religion_jewish', 'annotator_religion_mormon', 'annotator_religion_muslim', 'annotator_religion_nothing', 'annotator_religion_other', 'annotator_sexuality_bisexual', 'annotator_sexuality_gay', 'annotator_sexuality_straight', 'annotator_sexuality_other'],\n",
      "        num_rows: 108444\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['comment_id', 'annotator_id', 'platform', 'sentiment', 'respect', 'insult', 'humiliate', 'status', 'dehumanize', 'violence', 'genocide', 'attack_defend', 'hatespeech', 'hate_speech_score', 'text', 'infitms', 'outfitms', 'annotator_severity', 'std_err', 'annotator_infitms', 'annotator_outfitms', 'hypothesis', 'target_race_asian', 'target_race_black', 'target_race_latinx', 'target_race_middle_eastern', 'target_race_native_american', 'target_race_pacific_islander', 'target_race_white', 'target_race_other', 'target_race', 'target_religion_atheist', 'target_religion_buddhist', 'target_religion_christian', 'target_religion_hindu', 'target_religion_jewish', 'target_religion_mormon', 'target_religion_muslim', 'target_religion_other', 'target_religion', 'target_origin_immigrant', 'target_origin_migrant_worker', 'target_origin_specific_country', 'target_origin_undocumented', 'target_origin_other', 'target_origin', 'target_gender_men', 'target_gender_non_binary', 'target_gender_transgender_men', 'target_gender_transgender_unspecified', 'target_gender_transgender_women', 'target_gender_women', 'target_gender_other', 'target_gender', 'target_sexuality_bisexual', 'target_sexuality_gay', 'target_sexuality_lesbian', 'target_sexuality_straight', 'target_sexuality_other', 'target_sexuality', 'target_age_children', 'target_age_teenagers', 'target_age_young_adults', 'target_age_middle_aged', 'target_age_seniors', 'target_age_other', 'target_age', 'target_disability_physical', 'target_disability_cognitive', 'target_disability_neurological', 'target_disability_visually_impaired', 'target_disability_hearing_impaired', 'target_disability_unspecific', 'target_disability_other', 'target_disability', 'annotator_gender', 'annotator_trans', 'annotator_educ', 'annotator_income', 'annotator_ideology', 'annotator_gender_men', 'annotator_gender_women', 'annotator_gender_non_binary', 'annotator_gender_prefer_not_to_say', 'annotator_gender_self_describe', 'annotator_transgender', 'annotator_cisgender', 'annotator_transgender_prefer_not_to_say', 'annotator_education_some_high_school', 'annotator_education_high_school_grad', 'annotator_education_some_college', 'annotator_education_college_grad_aa', 'annotator_education_college_grad_ba', 'annotator_education_professional_degree', 'annotator_education_masters', 'annotator_education_phd', 'annotator_income_<10k', 'annotator_income_10k-50k', 'annotator_income_50k-100k', 'annotator_income_100k-200k', 'annotator_income_>200k', 'annotator_ideology_extremeley_conservative', 'annotator_ideology_conservative', 'annotator_ideology_slightly_conservative', 'annotator_ideology_neutral', 'annotator_ideology_slightly_liberal', 'annotator_ideology_liberal', 'annotator_ideology_extremeley_liberal', 'annotator_ideology_no_opinion', 'annotator_race_asian', 'annotator_race_black', 'annotator_race_latinx', 'annotator_race_middle_eastern', 'annotator_race_native_american', 'annotator_race_pacific_islander', 'annotator_race_white', 'annotator_race_other', 'annotator_age', 'annotator_religion_atheist', 'annotator_religion_buddhist', 'annotator_religion_christian', 'annotator_religion_hindu', 'annotator_religion_jewish', 'annotator_religion_mormon', 'annotator_religion_muslim', 'annotator_religion_nothing', 'annotator_religion_other', 'annotator_sexuality_bisexual', 'annotator_sexuality_gay', 'annotator_sexuality_straight', 'annotator_sexuality_other'],\n",
      "        num_rows: 13556\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['comment_id', 'annotator_id', 'platform', 'sentiment', 'respect', 'insult', 'humiliate', 'status', 'dehumanize', 'violence', 'genocide', 'attack_defend', 'hatespeech', 'hate_speech_score', 'text', 'infitms', 'outfitms', 'annotator_severity', 'std_err', 'annotator_infitms', 'annotator_outfitms', 'hypothesis', 'target_race_asian', 'target_race_black', 'target_race_latinx', 'target_race_middle_eastern', 'target_race_native_american', 'target_race_pacific_islander', 'target_race_white', 'target_race_other', 'target_race', 'target_religion_atheist', 'target_religion_buddhist', 'target_religion_christian', 'target_religion_hindu', 'target_religion_jewish', 'target_religion_mormon', 'target_religion_muslim', 'target_religion_other', 'target_religion', 'target_origin_immigrant', 'target_origin_migrant_worker', 'target_origin_specific_country', 'target_origin_undocumented', 'target_origin_other', 'target_origin', 'target_gender_men', 'target_gender_non_binary', 'target_gender_transgender_men', 'target_gender_transgender_unspecified', 'target_gender_transgender_women', 'target_gender_women', 'target_gender_other', 'target_gender', 'target_sexuality_bisexual', 'target_sexuality_gay', 'target_sexuality_lesbian', 'target_sexuality_straight', 'target_sexuality_other', 'target_sexuality', 'target_age_children', 'target_age_teenagers', 'target_age_young_adults', 'target_age_middle_aged', 'target_age_seniors', 'target_age_other', 'target_age', 'target_disability_physical', 'target_disability_cognitive', 'target_disability_neurological', 'target_disability_visually_impaired', 'target_disability_hearing_impaired', 'target_disability_unspecific', 'target_disability_other', 'target_disability', 'annotator_gender', 'annotator_trans', 'annotator_educ', 'annotator_income', 'annotator_ideology', 'annotator_gender_men', 'annotator_gender_women', 'annotator_gender_non_binary', 'annotator_gender_prefer_not_to_say', 'annotator_gender_self_describe', 'annotator_transgender', 'annotator_cisgender', 'annotator_transgender_prefer_not_to_say', 'annotator_education_some_high_school', 'annotator_education_high_school_grad', 'annotator_education_some_college', 'annotator_education_college_grad_aa', 'annotator_education_college_grad_ba', 'annotator_education_professional_degree', 'annotator_education_masters', 'annotator_education_phd', 'annotator_income_<10k', 'annotator_income_10k-50k', 'annotator_income_50k-100k', 'annotator_income_100k-200k', 'annotator_income_>200k', 'annotator_ideology_extremeley_conservative', 'annotator_ideology_conservative', 'annotator_ideology_slightly_conservative', 'annotator_ideology_neutral', 'annotator_ideology_slightly_liberal', 'annotator_ideology_liberal', 'annotator_ideology_extremeley_liberal', 'annotator_ideology_no_opinion', 'annotator_race_asian', 'annotator_race_black', 'annotator_race_latinx', 'annotator_race_middle_eastern', 'annotator_race_native_american', 'annotator_race_pacific_islander', 'annotator_race_white', 'annotator_race_other', 'annotator_age', 'annotator_religion_atheist', 'annotator_religion_buddhist', 'annotator_religion_christian', 'annotator_religion_hindu', 'annotator_religion_jewish', 'annotator_religion_mormon', 'annotator_religion_muslim', 'annotator_religion_nothing', 'annotator_religion_other', 'annotator_sexuality_bisexual', 'annotator_sexuality_gay', 'annotator_sexuality_straight', 'annotator_sexuality_other'],\n",
      "        num_rows: 13556\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_testvalid = hate['train'].train_test_split(test_size=0.2)\n",
    "# Split the 10% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "hate = datasets.DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "\n",
    "print(hate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2.73: 1284,\n",
       "         2.14: 1265,\n",
       "         2.1: 1116,\n",
       "         2.27: 958,\n",
       "         0.12: 785,\n",
       "         1.48: 749,\n",
       "         1.47: 731,\n",
       "         1.63: 725,\n",
       "         1.27: 723,\n",
       "         -0.94: 705,\n",
       "         2.45: 691,\n",
       "         -1.89: 686,\n",
       "         -3.01: 679,\n",
       "         1.57: 678,\n",
       "         -1.5: 668,\n",
       "         2.54: 668,\n",
       "         2.66: 663,\n",
       "         3.62: 660,\n",
       "         -2.77: 657,\n",
       "         1.96: 657,\n",
       "         2.36: 656,\n",
       "         3.91: 652,\n",
       "         -4.3: 652,\n",
       "         -4.44: 651,\n",
       "         -4.41: 651,\n",
       "         2.01: 650,\n",
       "         1.42: 645,\n",
       "         -5.4: 635,\n",
       "         2.97: 630,\n",
       "         3.44: 628,\n",
       "         -2.34: 625,\n",
       "         3.16: 623,\n",
       "         0.14: 620,\n",
       "         3.13: 616,\n",
       "         3.53: 616,\n",
       "         3.42: 612,\n",
       "         -2.43: 609,\n",
       "         -3.98: 606,\n",
       "         -2.36: 599,\n",
       "         -4.8: 589,\n",
       "         -5.43: 560,\n",
       "         -4.86: 555,\n",
       "         -2.61: 523,\n",
       "         -2.35: 512,\n",
       "         -0.42: 485,\n",
       "         -0.23: 481,\n",
       "         -5.12: 469,\n",
       "         -0.96: 457,\n",
       "         0.35: 443,\n",
       "         -0.87: 436,\n",
       "         0.46: 428,\n",
       "         0.13: 426,\n",
       "         1.44: 425,\n",
       "         -0.12: 421,\n",
       "         -0.92: 418,\n",
       "         1.7: 416,\n",
       "         -1.52: 390,\n",
       "         1.79: 388,\n",
       "         1.61: 386,\n",
       "         1.56: 379,\n",
       "         1.52: 378,\n",
       "         1.93: 375,\n",
       "         1.51: 365,\n",
       "         -1.85: 360,\n",
       "         1.32: 350,\n",
       "         -2.17: 277,\n",
       "         -0.43: 197,\n",
       "         -0.29: 190,\n",
       "         -0.22: 185,\n",
       "         -0.8: 183,\n",
       "         -0.52: 182,\n",
       "         -0.35: 182,\n",
       "         0.5: 181,\n",
       "         -0.76: 181,\n",
       "         -0.64: 178,\n",
       "         -0.15: 177,\n",
       "         0.25: 174,\n",
       "         -0.31: 173,\n",
       "         0.15: 173,\n",
       "         0.31: 173,\n",
       "         0.1: 172,\n",
       "         -0.68: 172,\n",
       "         -1.09: 170,\n",
       "         -0.05: 170,\n",
       "         -0.03: 169,\n",
       "         -0.45: 169,\n",
       "         0.81: 167,\n",
       "         0.51: 167,\n",
       "         0.58: 167,\n",
       "         -0.38: 167,\n",
       "         -0.71: 165,\n",
       "         -0.48: 165,\n",
       "         0.57: 165,\n",
       "         -0.3: 164,\n",
       "         -0.04: 164,\n",
       "         0.21: 163,\n",
       "         -0.74: 163,\n",
       "         -0.2: 162,\n",
       "         0.37: 162,\n",
       "         0.65: 161,\n",
       "         -0.7: 161,\n",
       "         -0.6: 161,\n",
       "         0.48: 161,\n",
       "         -0.06: 160,\n",
       "         -0.24: 160,\n",
       "         0.09: 159,\n",
       "         -0.14: 159,\n",
       "         -0.32: 159,\n",
       "         0.44: 158,\n",
       "         -0.33: 158,\n",
       "         0.38: 158,\n",
       "         0.72: 157,\n",
       "         0.0: 157,\n",
       "         -0.28: 156,\n",
       "         -0.27: 156,\n",
       "         0.34: 156,\n",
       "         0.68: 156,\n",
       "         0.78: 156,\n",
       "         0.28: 154,\n",
       "         -0.82: 154,\n",
       "         0.23: 154,\n",
       "         0.06: 154,\n",
       "         0.4: 154,\n",
       "         0.55: 153,\n",
       "         -0.26: 152,\n",
       "         0.95: 152,\n",
       "         0.62: 152,\n",
       "         1.02: 152,\n",
       "         -0.1: 150,\n",
       "         0.03: 150,\n",
       "         -1.02: 149,\n",
       "         0.22: 148,\n",
       "         0.41: 148,\n",
       "         -0.18: 148,\n",
       "         -0.57: 148,\n",
       "         0.67: 148,\n",
       "         0.89: 148,\n",
       "         0.02: 148,\n",
       "         0.19: 148,\n",
       "         -0.08: 148,\n",
       "         -0.63: 147,\n",
       "         0.61: 147,\n",
       "         0.74: 146,\n",
       "         -0.69: 146,\n",
       "         0.36: 146,\n",
       "         0.59: 146,\n",
       "         -0.46: 146,\n",
       "         -0.9: 145,\n",
       "         1.33: 144,\n",
       "         0.93: 144,\n",
       "         1.03: 144,\n",
       "         0.18: 144,\n",
       "         -0.86: 143,\n",
       "         -0.09: 143,\n",
       "         0.07: 143,\n",
       "         -0.67: 143,\n",
       "         0.33: 143,\n",
       "         -0.47: 142,\n",
       "         0.43: 142,\n",
       "         -1.57: 142,\n",
       "         -1.51: 142,\n",
       "         0.04: 142,\n",
       "         -0.79: 141,\n",
       "         -1.08: 141,\n",
       "         0.24: 141,\n",
       "         -0.58: 141,\n",
       "         -0.21: 141,\n",
       "         -0.97: 141,\n",
       "         -0.11: 141,\n",
       "         0.56: 141,\n",
       "         -0.55: 140,\n",
       "         0.47: 140,\n",
       "         -1.1: 140,\n",
       "         -1.0: 140,\n",
       "         0.71: 140,\n",
       "         -0.51: 140,\n",
       "         -0.53: 139,\n",
       "         -0.56: 139,\n",
       "         0.88: 139,\n",
       "         0.39: 139,\n",
       "         -0.37: 139,\n",
       "         0.2: 139,\n",
       "         0.8: 139,\n",
       "         0.05: 139,\n",
       "         0.86: 138,\n",
       "         -0.95: 137,\n",
       "         -0.72: 137,\n",
       "         -0.17: 137,\n",
       "         -1.06: 137,\n",
       "         1.08: 137,\n",
       "         -0.83: 137,\n",
       "         -0.77: 137,\n",
       "         -0.5: 136,\n",
       "         0.75: 136,\n",
       "         0.08: 136,\n",
       "         -1.3: 136,\n",
       "         -0.89: 135,\n",
       "         0.53: 135,\n",
       "         -0.44: 135,\n",
       "         0.77: 135,\n",
       "         -0.93: 135,\n",
       "         0.16: 135,\n",
       "         -0.78: 134,\n",
       "         -0.07: 134,\n",
       "         -0.02: 133,\n",
       "         0.76: 133,\n",
       "         0.45: 133,\n",
       "         0.49: 133,\n",
       "         -0.85: 133,\n",
       "         -1.47: 133,\n",
       "         0.87: 133,\n",
       "         1.06: 132,\n",
       "         -1.2: 132,\n",
       "         -0.34: 132,\n",
       "         1.17: 132,\n",
       "         0.27: 132,\n",
       "         1.14: 132,\n",
       "         0.7: 131,\n",
       "         1.07: 131,\n",
       "         -0.62: 131,\n",
       "         -0.66: 131,\n",
       "         1.31: 130,\n",
       "         -1.23: 130,\n",
       "         -1.83: 130,\n",
       "         -1.36: 130,\n",
       "         -0.16: 130,\n",
       "         0.3: 129,\n",
       "         -0.13: 129,\n",
       "         -0.73: 129,\n",
       "         -1.19: 129,\n",
       "         -1.32: 129,\n",
       "         -0.61: 128,\n",
       "         0.83: 128,\n",
       "         0.32: 128,\n",
       "         -2.37: 127,\n",
       "         0.29: 127,\n",
       "         -1.33: 127,\n",
       "         0.98: 126,\n",
       "         0.99: 126,\n",
       "         -1.03: 126,\n",
       "         0.69: 126,\n",
       "         -1.15: 126,\n",
       "         0.6: 126,\n",
       "         0.94: 125,\n",
       "         0.97: 125,\n",
       "         0.26: 125,\n",
       "         -0.75: 125,\n",
       "         -1.68: 125,\n",
       "         0.85: 125,\n",
       "         -0.88: 125,\n",
       "         0.52: 124,\n",
       "         1.23: 124,\n",
       "         0.79: 124,\n",
       "         -1.12: 124,\n",
       "         -0.39: 123,\n",
       "         0.9: 123,\n",
       "         -1.35: 123,\n",
       "         0.73: 123,\n",
       "         -0.81: 123,\n",
       "         -2.7: 122,\n",
       "         -2.33: 122,\n",
       "         -2.19: 122,\n",
       "         -0.4: 122,\n",
       "         0.82: 122,\n",
       "         0.11: 122,\n",
       "         -1.13: 122,\n",
       "         -1.7: 122,\n",
       "         0.17: 122,\n",
       "         0.92: 121,\n",
       "         1.0: 120,\n",
       "         -1.62: 120,\n",
       "         -1.53: 120,\n",
       "         -0.99: 119,\n",
       "         -0.25: 119,\n",
       "         1.26: 119,\n",
       "         1.15: 118,\n",
       "         -0.59: 118,\n",
       "         -1.92: 118,\n",
       "         -0.01: 118,\n",
       "         1.05: 118,\n",
       "         1.11: 118,\n",
       "         -0.41: 118,\n",
       "         -2.21: 118,\n",
       "         -1.25: 118,\n",
       "         -1.29: 118,\n",
       "         -0.54: 117,\n",
       "         -2.08: 117,\n",
       "         1.3: 117,\n",
       "         -0.49: 117,\n",
       "         -0.36: 117,\n",
       "         -1.46: 117,\n",
       "         -1.16: 116,\n",
       "         -1.79: 116,\n",
       "         0.96: 116,\n",
       "         -2.28: 116,\n",
       "         1.16: 116,\n",
       "         -1.18: 116,\n",
       "         -1.04: 116,\n",
       "         -1.24: 116,\n",
       "         0.66: 114,\n",
       "         1.45: 114,\n",
       "         0.91: 114,\n",
       "         -1.43: 114,\n",
       "         -2.11: 114,\n",
       "         -1.07: 114,\n",
       "         -1.65: 113,\n",
       "         -0.84: 113,\n",
       "         1.34: 113,\n",
       "         -1.11: 113,\n",
       "         -1.56: 113,\n",
       "         1.28: 112,\n",
       "         1.24: 112,\n",
       "         -1.42: 112,\n",
       "         -1.39: 112,\n",
       "         -0.98: 112,\n",
       "         -0.91: 112,\n",
       "         -0.65: 111,\n",
       "         -1.17: 111,\n",
       "         1.35: 111,\n",
       "         -1.73: 111,\n",
       "         -1.21: 111,\n",
       "         -2.92: 110,\n",
       "         1.01: 110,\n",
       "         1.04: 110,\n",
       "         1.39: 110,\n",
       "         -1.94: 110,\n",
       "         -1.58: 110,\n",
       "         -1.96: 109,\n",
       "         -3.18: 109,\n",
       "         -1.45: 109,\n",
       "         -2.65: 109,\n",
       "         1.09: 109,\n",
       "         1.49: 109,\n",
       "         1.53: 108,\n",
       "         -1.71: 108,\n",
       "         -1.28: 108,\n",
       "         -2.73: 108,\n",
       "         1.21: 108,\n",
       "         0.63: 108,\n",
       "         -1.78: 108,\n",
       "         1.4: 108,\n",
       "         -2.83: 108,\n",
       "         -1.98: 107,\n",
       "         0.42: 107,\n",
       "         0.01: 107,\n",
       "         -1.86: 107,\n",
       "         -2.46: 107,\n",
       "         -2.22: 107,\n",
       "         -1.01: 106,\n",
       "         0.64: 106,\n",
       "         -1.22: 106,\n",
       "         -2.27: 105,\n",
       "         -1.74: 105,\n",
       "         1.29: 105,\n",
       "         -2.56: 105,\n",
       "         1.43: 104,\n",
       "         1.55: 104,\n",
       "         1.19: 104,\n",
       "         -1.91: 103,\n",
       "         0.84: 103,\n",
       "         -1.54: 103,\n",
       "         -3.22: 103,\n",
       "         -1.48: 103,\n",
       "         -2.45: 103,\n",
       "         -2.24: 103,\n",
       "         -0.19: 103,\n",
       "         -1.55: 103,\n",
       "         -2.63: 102,\n",
       "         -1.37: 102,\n",
       "         -1.87: 102,\n",
       "         -2.74: 102,\n",
       "         -1.4: 102,\n",
       "         -1.75: 102,\n",
       "         -2.48: 101,\n",
       "         -2.41: 101,\n",
       "         1.6: 101,\n",
       "         -1.38: 101,\n",
       "         -2.68: 101,\n",
       "         0.54: 101,\n",
       "         -1.05: 100,\n",
       "         1.41: 100,\n",
       "         -1.6: 100,\n",
       "         -1.34: 100,\n",
       "         -1.66: 100,\n",
       "         -1.64: 100,\n",
       "         -2.85: 99,\n",
       "         -2.59: 99,\n",
       "         -3.11: 99,\n",
       "         -2.42: 99,\n",
       "         -2.0: 99,\n",
       "         -2.02: 98,\n",
       "         -1.26: 98,\n",
       "         -3.21: 98,\n",
       "         1.62: 97,\n",
       "         -2.96: 97,\n",
       "         -1.97: 97,\n",
       "         -1.49: 97,\n",
       "         -2.49: 97,\n",
       "         1.1: 96,\n",
       "         1.36: 96,\n",
       "         1.64: 96,\n",
       "         -2.32: 96,\n",
       "         -1.72: 96,\n",
       "         -2.1: 96,\n",
       "         -1.82: 95,\n",
       "         1.37: 95,\n",
       "         -1.27: 95,\n",
       "         -3.16: 95,\n",
       "         1.22: 95,\n",
       "         1.73: 95,\n",
       "         -2.64: 94,\n",
       "         -2.14: 94,\n",
       "         -2.54: 94,\n",
       "         1.25: 94,\n",
       "         1.18: 94,\n",
       "         -3.03: 94,\n",
       "         -2.2: 93,\n",
       "         1.54: 93,\n",
       "         -2.26: 93,\n",
       "         -2.5: 93,\n",
       "         -1.67: 93,\n",
       "         -1.95: 93,\n",
       "         -2.15: 92,\n",
       "         -1.8: 92,\n",
       "         -2.88: 92,\n",
       "         -1.99: 92,\n",
       "         -2.05: 92,\n",
       "         -1.61: 91,\n",
       "         -1.31: 91,\n",
       "         -2.01: 91,\n",
       "         -2.6: 91,\n",
       "         -2.47: 90,\n",
       "         -1.41: 90,\n",
       "         -1.93: 89,\n",
       "         -1.77: 89,\n",
       "         1.13: 88,\n",
       "         -2.09: 88,\n",
       "         -2.04: 88,\n",
       "         -3.07: 88,\n",
       "         -1.81: 88,\n",
       "         -1.84: 88,\n",
       "         1.2: 88,\n",
       "         -1.63: 88,\n",
       "         -1.44: 88,\n",
       "         -2.07: 87,\n",
       "         -2.23: 87,\n",
       "         -3.08: 87,\n",
       "         -1.76: 87,\n",
       "         -2.13: 87,\n",
       "         1.65: 86,\n",
       "         1.77: 86,\n",
       "         -2.31: 86,\n",
       "         -2.89: 86,\n",
       "         -3.12: 86,\n",
       "         -2.57: 86,\n",
       "         -3.23: 85,\n",
       "         -2.91: 85,\n",
       "         1.12: 85,\n",
       "         -2.69: 85,\n",
       "         1.72: 85,\n",
       "         -1.9: 85,\n",
       "         -1.14: 85,\n",
       "         -2.75: 84,\n",
       "         -2.51: 83,\n",
       "         -2.4: 83,\n",
       "         1.94: 83,\n",
       "         -2.16: 82,\n",
       "         -2.76: 82,\n",
       "         -2.38: 82,\n",
       "         -2.3: 81,\n",
       "         -2.78: 81,\n",
       "         -2.55: 81,\n",
       "         -1.59: 81,\n",
       "         -2.87: 80,\n",
       "         -3.54: 80,\n",
       "         1.91: 80,\n",
       "         1.66: 80,\n",
       "         -2.12: 80,\n",
       "         -2.93: 80,\n",
       "         1.98: 80,\n",
       "         -2.53: 79,\n",
       "         -2.39: 79,\n",
       "         -3.0: 79,\n",
       "         -2.44: 79,\n",
       "         1.74: 79,\n",
       "         -1.69: 78,\n",
       "         -2.99: 78,\n",
       "         -2.25: 78,\n",
       "         -2.71: 78,\n",
       "         1.58: 78,\n",
       "         -2.67: 78,\n",
       "         1.69: 77,\n",
       "         -3.41: 77,\n",
       "         -4.0: 77,\n",
       "         -3.59: 77,\n",
       "         -2.9: 77,\n",
       "         -2.58: 76,\n",
       "         -3.72: 76,\n",
       "         -2.72: 76,\n",
       "         -3.05: 75,\n",
       "         -3.15: 75,\n",
       "         -2.79: 75,\n",
       "         1.85: 75,\n",
       "         -3.24: 74,\n",
       "         -3.42: 74,\n",
       "         -2.29: 74,\n",
       "         2.06: 74,\n",
       "         -2.8: 74,\n",
       "         -3.28: 74,\n",
       "         -2.81: 74,\n",
       "         1.38: 74,\n",
       "         -3.71: 74,\n",
       "         1.76: 73,\n",
       "         1.75: 73,\n",
       "         1.8: 73,\n",
       "         -3.44: 73,\n",
       "         -3.27: 72,\n",
       "         1.46: 72,\n",
       "         -3.3: 72,\n",
       "         -2.06: 71,\n",
       "         2.11: 71,\n",
       "         -3.47: 71,\n",
       "         -3.43: 71,\n",
       "         -3.39: 71,\n",
       "         -3.9: 71,\n",
       "         -3.09: 71,\n",
       "         -3.19: 71,\n",
       "         -3.02: 71,\n",
       "         1.71: 70,\n",
       "         -2.18: 70,\n",
       "         1.5: 69,\n",
       "         -3.37: 69,\n",
       "         1.59: 69,\n",
       "         -2.95: 69,\n",
       "         -2.52: 69,\n",
       "         -3.26: 69,\n",
       "         -3.04: 68,\n",
       "         -3.86: 68,\n",
       "         -2.62: 68,\n",
       "         -3.52: 68,\n",
       "         1.9: 68,\n",
       "         -2.86: 68,\n",
       "         -3.31: 68,\n",
       "         -3.13: 67,\n",
       "         -4.07: 66,\n",
       "         -3.84: 66,\n",
       "         1.86: 66,\n",
       "         1.97: 66,\n",
       "         -3.55: 66,\n",
       "         1.95: 66,\n",
       "         -3.87: 66,\n",
       "         1.81: 65,\n",
       "         -3.38: 65,\n",
       "         -3.06: 65,\n",
       "         2.02: 65,\n",
       "         -3.61: 65,\n",
       "         -2.98: 64,\n",
       "         -3.25: 64,\n",
       "         -3.17: 64,\n",
       "         -3.45: 64,\n",
       "         1.67: 63,\n",
       "         -2.84: 63,\n",
       "         1.89: 63,\n",
       "         -4.37: 63,\n",
       "         -3.1: 62,\n",
       "         -2.97: 62,\n",
       "         -3.66: 62,\n",
       "         1.92: 62,\n",
       "         -3.76: 61,\n",
       "         -3.82: 61,\n",
       "         -3.65: 61,\n",
       "         -1.88: 61,\n",
       "         1.68: 61,\n",
       "         -3.46: 61,\n",
       "         -2.94: 60,\n",
       "         -3.34: 60,\n",
       "         -3.64: 60,\n",
       "         -3.62: 60,\n",
       "         -4.38: 60,\n",
       "         -3.6: 59,\n",
       "         -2.03: 59,\n",
       "         -3.36: 59,\n",
       "         -3.53: 59,\n",
       "         1.78: 58,\n",
       "         2.2: 58,\n",
       "         1.83: 57,\n",
       "         1.84: 57,\n",
       "         -3.97: 56,\n",
       "         1.99: 56,\n",
       "         2.18: 56,\n",
       "         -3.85: 56,\n",
       "         -3.7: 56,\n",
       "         -4.18: 56,\n",
       "         -3.48: 55,\n",
       "         -4.06: 55,\n",
       "         -2.82: 55,\n",
       "         -4.5: 54,\n",
       "         -3.75: 54,\n",
       "         -3.88: 54,\n",
       "         -4.17: 54,\n",
       "         2.0: 54,\n",
       "         -3.89: 53,\n",
       "         -3.68: 53,\n",
       "         1.82: 53,\n",
       "         2.03: 53,\n",
       "         2.3: 52,\n",
       "         -3.77: 52,\n",
       "         -4.03: 52,\n",
       "         -3.95: 52,\n",
       "         -3.79: 52,\n",
       "         -4.14: 52,\n",
       "         -4.24: 52,\n",
       "         -4.2: 51,\n",
       "         2.07: 51,\n",
       "         -4.21: 51,\n",
       "         -2.66: 51,\n",
       "         -3.2: 51,\n",
       "         -4.15: 50,\n",
       "         -3.5: 50,\n",
       "         -3.14: 50,\n",
       "         -4.23: 50,\n",
       "         2.15: 50,\n",
       "         2.25: 49,\n",
       "         -3.69: 49,\n",
       "         -4.66: 49,\n",
       "         2.31: 49,\n",
       "         -4.55: 49,\n",
       "         -3.92: 49,\n",
       "         1.88: 49,\n",
       "         -3.74: 49,\n",
       "         -4.36: 48,\n",
       "         -4.42: 48,\n",
       "         2.32: 48,\n",
       "         -3.83: 48,\n",
       "         -4.1: 48,\n",
       "         -4.63: 48,\n",
       "         -4.57: 48,\n",
       "         -3.67: 47,\n",
       "         -3.49: 47,\n",
       "         2.05: 47,\n",
       "         2.26: 47,\n",
       "         2.12: 47,\n",
       "         2.08: 46,\n",
       "         -4.88: 46,\n",
       "         2.28: 46,\n",
       "         -3.57: 46,\n",
       "         -4.45: 46,\n",
       "         2.24: 46,\n",
       "         -3.4: 45,\n",
       "         -3.32: 45,\n",
       "         -4.05: 45,\n",
       "         2.34: 45,\n",
       "         -3.29: 45,\n",
       "         -4.64: 44,\n",
       "         2.21: 44,\n",
       "         -4.82: 44,\n",
       "         -3.73: 44,\n",
       "         -4.59: 44,\n",
       "         -4.09: 44,\n",
       "         2.37: 44,\n",
       "         -3.8: 44,\n",
       "         -3.78: 43,\n",
       "         -4.32: 43,\n",
       "         -4.19: 43,\n",
       "         -4.49: 43,\n",
       "         -3.33: 42,\n",
       "         -3.58: 42,\n",
       "         -4.27: 42,\n",
       "         1.87: 42,\n",
       "         -3.63: 42,\n",
       "         -4.01: 41,\n",
       "         -4.68: 41,\n",
       "         -3.51: 41,\n",
       "         -3.81: 41,\n",
       "         -4.31: 41,\n",
       "         -4.74: 40,\n",
       "         2.19: 40,\n",
       "         -4.11: 40,\n",
       "         2.16: 40,\n",
       "         -4.04: 40,\n",
       "         -4.81: 40,\n",
       "         -4.02: 39,\n",
       "         -3.93: 39,\n",
       "         -4.71: 39,\n",
       "         -4.12: 38,\n",
       "         -4.4: 38,\n",
       "         2.13: 38,\n",
       "         -4.46: 38,\n",
       "         2.23: 38,\n",
       "         -4.26: 38,\n",
       "         -4.22: 38,\n",
       "         -3.94: 38,\n",
       "         2.43: 37,\n",
       "         2.17: 37,\n",
       "         -4.93: 37,\n",
       "         -4.7: 37,\n",
       "         -4.13: 36,\n",
       "         2.72: 36,\n",
       "         -3.56: 36,\n",
       "         -5.36: 36,\n",
       "         -3.99: 36,\n",
       "         -4.28: 36,\n",
       "         -4.77: 36,\n",
       "         -4.29: 36,\n",
       "         -4.84: 35,\n",
       "         -4.72: 35,\n",
       "         2.04: 35,\n",
       "         -4.08: 34,\n",
       "         2.42: 34,\n",
       "         -5.04: 34,\n",
       "         -5.39: 34,\n",
       "         -5.26: 34,\n",
       "         -4.47: 34,\n",
       "         -4.33: 34,\n",
       "         -3.91: 33,\n",
       "         2.5: 33,\n",
       "         -4.53: 33,\n",
       "         -4.58: 33,\n",
       "         -5.74: 33,\n",
       "         -4.16: 33,\n",
       "         2.39: 32,\n",
       "         -4.39: 32,\n",
       "         -5.1: 32,\n",
       "         -4.52: 32,\n",
       "         -5.01: 32,\n",
       "         -4.75: 31,\n",
       "         -5.06: 31,\n",
       "         -5.07: 31,\n",
       "         2.09: 30,\n",
       "         -4.92: 30,\n",
       "         -3.35: 30,\n",
       "         -4.91: 30,\n",
       "         2.41: 30,\n",
       "         -4.65: 29,\n",
       "         -5.19: 29,\n",
       "         -4.25: 29,\n",
       "         -4.54: 29,\n",
       "         -4.99: 29,\n",
       "         2.29: 29,\n",
       "         -5.02: 29,\n",
       "         -4.35: 28,\n",
       "         2.68: 28,\n",
       "         -5.5: 28,\n",
       "         -4.98: 28,\n",
       "         -4.95: 28,\n",
       "         2.64: 28,\n",
       "         -5.08: 28,\n",
       "         2.38: 28,\n",
       "         2.49: 27,\n",
       "         -4.96: 27,\n",
       "         2.76: 27,\n",
       "         -4.43: 27,\n",
       "         2.57: 27,\n",
       "         2.33: 27,\n",
       "         -4.97: 26,\n",
       "         -4.34: 26,\n",
       "         -5.47: 26,\n",
       "         -3.96: 26,\n",
       "         2.82: 26,\n",
       "         -5.6: 26,\n",
       "         -4.67: 26,\n",
       "         2.48: 25,\n",
       "         -5.27: 25,\n",
       "         2.4: 25,\n",
       "         -5.25: 25,\n",
       "         -5.15: 25,\n",
       "         -4.78: 25,\n",
       "         -4.51: 25,\n",
       "         -5.51: 25,\n",
       "         -4.62: 24,\n",
       "         -5.2: 24,\n",
       "         -4.61: 24,\n",
       "         -4.56: 24,\n",
       "         -5.42: 24,\n",
       "         -4.94: 24,\n",
       "         2.65: 24,\n",
       "         -5.28: 24,\n",
       "         2.79: 24,\n",
       "         2.51: 23,\n",
       "         2.58: 23,\n",
       "         2.92: 23,\n",
       "         -5.18: 23,\n",
       "         2.47: 23,\n",
       "         -5.41: 23,\n",
       "         -5.16: 23,\n",
       "         -5.11: 23,\n",
       "         2.84: 23,\n",
       "         -4.89: 23,\n",
       "         -4.76: 23,\n",
       "         -4.6: 23,\n",
       "         -4.85: 23,\n",
       "         -5.57: 22,\n",
       "         -5.32: 22,\n",
       "         -5.48: 22,\n",
       "         -5.52: 22,\n",
       "         2.53: 22,\n",
       "         -4.83: 21,\n",
       "         -5.23: 21,\n",
       "         -4.69: 21,\n",
       "         -5.14: 21,\n",
       "         -5.56: 21,\n",
       "         3.08: 21,\n",
       "         2.56: 21,\n",
       "         -5.0: 21,\n",
       "         -5.49: 21,\n",
       "         -5.03: 21,\n",
       "         2.35: 20,\n",
       "         2.7: 20,\n",
       "         -5.17: 20,\n",
       "         -5.21: 20,\n",
       "         -5.58: 20,\n",
       "         -5.31: 20,\n",
       "         -6.34: 20,\n",
       "         -5.61: 19,\n",
       "         -4.48: 19,\n",
       "         -5.34: 19,\n",
       "         -5.38: 19,\n",
       "         2.77: 19,\n",
       "         -5.44: 19,\n",
       "         2.63: 19,\n",
       "         -6.52: 19,\n",
       "         -4.79: 19,\n",
       "         3.29: 18,\n",
       "         3.1: 18,\n",
       "         2.74: 18,\n",
       "         -5.78: 18,\n",
       "         3.05: 18,\n",
       "         2.85: 18,\n",
       "         -5.91: 18,\n",
       "         -5.76: 18,\n",
       "         2.71: 17,\n",
       "         -5.65: 17,\n",
       "         2.22: 17,\n",
       "         -5.59: 17,\n",
       "         -5.9: 17,\n",
       "         -5.24: 17,\n",
       "         -6.43: 17,\n",
       "         -5.69: 17,\n",
       "         -5.05: 17,\n",
       "         2.87: 17,\n",
       "         2.52: 17,\n",
       "         -5.64: 17,\n",
       "         -5.72: 17,\n",
       "         -5.55: 16,\n",
       "         2.98: 16,\n",
       "         2.78: 16,\n",
       "         -5.35: 16,\n",
       "         2.61: 16,\n",
       "         -4.9: 16,\n",
       "         -5.63: 16,\n",
       "         -5.46: 16,\n",
       "         2.75: 15,\n",
       "         -5.67: 15,\n",
       "         -5.96: 15,\n",
       "         2.46: 15,\n",
       "         -6.9: 15,\n",
       "         2.55: 15,\n",
       "         3.37: 15,\n",
       "         -6.64: 15,\n",
       "         -5.13: 15,\n",
       "         2.8: 15,\n",
       "         2.93: 15,\n",
       "         -4.73: 15,\n",
       "         -6.05: 15,\n",
       "         -5.33: 15,\n",
       "         -5.45: 15,\n",
       "         2.95: 15,\n",
       "         -5.29: 15,\n",
       "         2.59: 15,\n",
       "         2.6: 14,\n",
       "         -6.0: 14,\n",
       "         -6.04: 14,\n",
       "         2.86: 14,\n",
       "         3.09: 14,\n",
       "         2.99: 14,\n",
       "         3.33: 13,\n",
       "         -5.95: 13,\n",
       "         -5.85: 13,\n",
       "         -5.83: 13,\n",
       "         2.9: 13,\n",
       "         3.52: 13,\n",
       "         2.88: 12,\n",
       "         -5.97: 12,\n",
       "         -6.35: 12,\n",
       "         3.18: 12,\n",
       "         3.24: 12,\n",
       "         -7.9: 12,\n",
       "         -6.06: 12,\n",
       "         3.11: 12,\n",
       "         -6.57: 12,\n",
       "         -7.0: 12,\n",
       "         -6.08: 12,\n",
       "         -5.92: 12,\n",
       "         2.91: 12,\n",
       "         -5.3: 12,\n",
       "         2.81: 12,\n",
       "         3.07: 12,\n",
       "         -5.22: 12,\n",
       "         3.47: 11,\n",
       "         3.0: 11,\n",
       "         -6.23: 11,\n",
       "         3.03: 11,\n",
       "         2.69: 11,\n",
       "         -5.81: 11,\n",
       "         -6.76: 11,\n",
       "         -6.86: 11,\n",
       "         -6.02: 11,\n",
       "         3.32: 11,\n",
       "         -6.16: 11,\n",
       "         -6.25: 11,\n",
       "         -5.66: 11,\n",
       "         -5.53: 11,\n",
       "         -6.29: 11,\n",
       "         2.67: 10,\n",
       "         -5.88: 10,\n",
       "         -6.54: 10,\n",
       "         -7.36: 10,\n",
       "         -5.98: 10,\n",
       "         -6.03: 10,\n",
       "         2.89: 10,\n",
       "         2.62: 10,\n",
       "         -5.87: 10,\n",
       "         3.17: 10,\n",
       "         -7.14: 10,\n",
       "         -5.8: 10,\n",
       "         -5.37: 10,\n",
       "         3.21: 10,\n",
       "         -6.84: 10,\n",
       "         -5.79: 10,\n",
       "         2.44: 10,\n",
       "         -6.66: 10,\n",
       "         3.04: 10,\n",
       "         -5.7: 10,\n",
       "         2.96: 10,\n",
       "         -5.54: 10,\n",
       "         -6.27: 9,\n",
       "         2.83: 9,\n",
       "         -6.59: 9,\n",
       "         -5.77: 9,\n",
       "         -5.99: 9,\n",
       "         -6.95: 9,\n",
       "         -7.73: 9,\n",
       "         -6.77: 9,\n",
       "         -6.07: 9,\n",
       "         -6.31: 9,\n",
       "         -6.71: 9,\n",
       "         3.89: 9,\n",
       "         -6.14: 9,\n",
       "         -6.13: 9,\n",
       "         -6.48: 9,\n",
       "         -7.07: 9,\n",
       "         -6.24: 9,\n",
       "         -5.75: 9,\n",
       "         -6.2: 9,\n",
       "         3.83: 8,\n",
       "         4.77: 8,\n",
       "         -6.28: 8,\n",
       "         -5.62: 8,\n",
       "         3.4: 8,\n",
       "         -7.56: 8,\n",
       "         -7.35: 8,\n",
       "         -6.3: 8,\n",
       "         -6.38: 8,\n",
       "         3.58: 8,\n",
       "         -7.94: 8,\n",
       "         -7.74: 8,\n",
       "         -5.82: 8,\n",
       "         3.54: 8,\n",
       "         -6.46: 8,\n",
       "         -6.44: 8,\n",
       "         3.26: 8,\n",
       "         -6.39: 8,\n",
       "         -4.87: 8,\n",
       "         -7.02: 8,\n",
       "         -7.77: 8,\n",
       "         3.9: 7,\n",
       "         -6.37: 7,\n",
       "         3.98: 7,\n",
       "         3.38: 7,\n",
       "         -6.6: 7,\n",
       "         -5.84: 7,\n",
       "         -5.86: 7,\n",
       "         3.57: 7,\n",
       "         -6.61: 7,\n",
       "         3.36: 7,\n",
       "         -7.44: 7,\n",
       "         2.94: 7,\n",
       "         -6.11: 7,\n",
       "         -7.89: 7,\n",
       "         -6.98: 7,\n",
       "         -5.71: 7,\n",
       "         -6.36: 7,\n",
       "         4.01: 7,\n",
       "         -7.61: 7,\n",
       "         3.01: 6,\n",
       "         -6.17: 6,\n",
       "         -7.09: 6,\n",
       "         -7.47: 6,\n",
       "         3.64: 6,\n",
       "         -6.15: 6,\n",
       "         ...})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check class distribution in training and testing set\n",
    "\n",
    "Counter(hate[\"train\"][\"hate_speech_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_small = hate.select_columns([\"text\", \"hate_speech_score\"])\n",
    "hate_small = hate_small.rename_column(\"hate_speech_score\", \"label\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 108444/108444 [00:16<00:00, 6447.17 examples/s]\n",
      "Map: 100%|| 13556/13556 [00:02<00:00, 6621.06 examples/s]\n",
      "Map: 100%|| 13556/13556 [00:02<00:00, 6240.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_hate_small = hate_small.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 9.31 MiB is free. Including non-PyTorch memory, this process has 3.79 GiB memory in use. Of the allocated memory 3.66 GiB is allocated by PyTorch, and 44.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_hate_small\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_hate_small\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:489\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    488\u001b[0m ):\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:730\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 730\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/modeling_utils.py:2556\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2554\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2555\u001b[0m         )\n\u001b[0;32m-> 2556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 9.31 MiB is free. Including non-PyTorch memory, this process has 3.79 GiB memory in use. Of the allocated memory 3.66 GiB is allocated by PyTorch, and 44.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_hate_small[\"train\"],\n",
    "    eval_dataset=tokenized_hate_small[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40668 [01:15<?, ?it/s]\n",
      "  0%|          | 0/40668 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 9.31 MiB is free. Including non-PyTorch memory, this process has 3.79 GiB memory in use. Of the allocated memory 3.66 GiB is allocated by PyTorch, and 41.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1967\u001b[0m ):\n\u001b[1;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:2902\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2901\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2902\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2905\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/trainer.py:2925\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2924\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2925\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2926\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2927\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1564\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1564\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1578\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    536\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    537\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 539\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/pytorch_utils.py:237\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    551\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 552\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:465\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    464\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 465\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/functional.py:1268\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 9.31 MiB is free. Including non-PyTorch memory, this process has 3.79 GiB memory in use. Of the allocated memory 3.66 GiB is allocated by PyTorch, and 41.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
